{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1:-\n",
    "\n",
    " Assumptions of ANOVA:\n",
    "\n",
    "Independence: Observations in each group are independent of each other.\n",
    "\n",
    "Normality: The dependent variable is normally distributed within each group.\n",
    "\n",
    "Homogeneity of variance: The variances of the dependent variable are equal across all groups.\n",
    "\n",
    "Violations that could impact the validity of the results:\n",
    "\n",
    "\n",
    "Violations of independence: If observations within a group are not independent of each other, the assumption of independence is violated. This can happen, for example, when a repeated measures design is used and observations within the same individual are not independent.\n",
    "\n",
    "Violations of normality: If the dependent variable is not normally distributed within each group, the assumption of normality is violated. This can happen when outliers are present or when the sample size is small.\n",
    "Violations of homogeneity of variance: If the variances of the dependent variable are not equal across all groups, the assumption of homogeneity of variance is violated. This can happen when one group has a much larger variance than the other groups, or when the sample sizes are different between groups.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2:-\n",
    "\n",
    " Three types of ANOVA:\n",
    "\n",
    "One-way ANOVA: used when comparing means of a single independent variable with more than two categories or levels.\n",
    "\n",
    "Two-way ANOVA: used when comparing means of two independent variables.\n",
    "\n",
    "N-way ANOVA: used when comparing means of more than two independent variables.\n",
    "\n",
    "In a one-way ANOVA, each level of the independent variable is compared against the grand mean. In a two-way ANOVA, the means of each level of the two independent variables are compared, as well as the interaction between the two variables.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q3:-\n",
    "\n",
    " Partitioning of variance in ANOVA:\n",
    "In ANOVA, the total variance in the dependent variable is partitioned into two components: the variance that can be explained by the independent variable(s) (called the explained variance), and the variance that cannot be explained by the independent variable(s) (called the residual variance). The proportion of the total variance that is explained by the independent variable(s) is called the effect size, and is a measure of the strength of the relationship between the independent and dependent variables.\n",
    "\n",
    "It is important to understand the partitioning of variance in ANOVA because it helps to identify the sources of variability in the data, and to determine the extent to which the independent variable(s) are associated with the dependent variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q4:-\n",
    "\n",
    " To calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python, you can use the `statsmodels` library. Here's an example code:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Fit ANOVA model\n",
    "model = ols('response ~ group', data=data).fit()\n",
    "\n",
    "# Calculate SST\n",
    "SST = sum((data['response'] - data['response'].mean()) ** 2)\n",
    "\n",
    "# Calculate SSE\n",
    "SSE = sum(model.resid ** 2)\n",
    "\n",
    "# Calculate SSR\n",
    "SSR = SST - SSE\n",
    "```\n",
    "\n",
    "In this code, `data` is a pandas DataFrame containing the data for the one-way ANOVA, with a column named `response` for the response variable and a column named `group` for the grouping variable. The `ols` function from `statsmodels` is used to fit the ANOVA model, and the `fit` method is used to obtain the residuals. Finally, SST, SSE, and SSR are calculated using the formulas for these statistics.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5:-\n",
    "\n",
    " In a two-way ANOVA, the main effects and interaction effects can be calculated using the `ols` function from `statsmodels` in Python. Here's an example code:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Fit ANOVA model\n",
    "model = ols('response ~ factorA * factorB', data=data).fit()\n",
    "\n",
    "# Calculate main effects\n",
    "main_effects = model.params.loc[['factorA', 'factorB']]\n",
    "\n",
    "# Calculate interaction effect\n",
    "interaction_effect = model.params['factorA:factorB']\n",
    "```\n",
    "\n",
    "In this code, `data` is a pandas DataFrame containing the data for the two-way ANOVA, with a column named `response` for the response variable and two columns named `factorA` and `factorB` for the two factors. The `ols` function from `statsmodels` is used to fit the ANOVA model, with the `*` operator indicating that both main effects and the interaction effect should be included in the model. The `params` attribute of the model is used to obtain the parameter estimates, from which the main effects and interaction effect can be calculated.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q6:-\n",
    "\n",
    " If you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02, you can conclude that there is a significant difference between the groups. Specifically, the null hypothesis that the means of all groups are equal is rejected at a significance level of 0.05. This means that at least one group has a different mean from the others.\n",
    "\n",
    "The F-statistic measures the ratio of the variance between groups to the variance within groups. A large F-statistic indicates that the between-group variance is large relative to the within-group variance, which suggests that there is a significant difference between the groups. The p-value measures the probability of obtaining an F-statistic as extreme or more extreme than the observed value under the null hypothesis. In this case, the p-value of 0.02 indicates that the observed F-statistic is unlikely to have occurred by chance under the null hypothesis, providing further evidence for rejecting the null hypothesis.\n",
    "\n",
    "In summary, the results of the one-way ANOVA suggest that there are significant differences between the groups, and follow-up analyses such as post-hoc tests may be conducted to determine which specific groups differ significantly from each other. It's important to note that ANOVA only tells us that there is a significant difference between the groups, but it doesn't tell us which group(s) is/are different. Post-hoc tests can be used to compare the means of individual groups and determine which ones differ significantly from each other.\n",
    "\n",
    "When interpreting the results of an ANOVA, it's also important to consider the effect size, which measures the magnitude of the difference between the groups. One commonly used effect size measure for ANOVA is eta-squared (η²), which represents the proportion of variance in the response variable that is explained by the grouping variable. In this case, an eta-squared value of 0.1, for example, would indicate that 10% of the variance in the response variable is explained by the grouping variable.\n",
    "\n",
    "Overall, the results of an ANOVA can provide valuable insights into the differences between groups and help identify factors that may be contributing to these differences. However, it's important to use caution when interpreting these results and to consider other factors such as effect size and potential confounding variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7:-\n",
    "\n",
    " In a repeated measures ANOVA, handling missing data can be challenging. One approach is to use imputation methods to replace missing values with estimated values. There are several methods of imputation, including mean imputation, regression imputation, and multiple imputation. However, it's important to note that different imputation methods can lead to different results, and the choice of method may depend on the nature of the missing data and the research question being addressed.\n",
    "\n",
    "Another approach is to exclude cases with missing data from the analysis. However, this can lead to a loss of power and potentially biased results if the missing data are related to the outcome variable or other variables in the model.\n",
    "\n",
    "The consequences of using different methods to handle missing data can be substantial, and it's important to carefully consider the implications of each approach before deciding on a course of action. It's also important to report any missing data and the methods used to handle them in the study report.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8:-\n",
    "\n",
    " Common post-hoc tests used after ANOVA include Tukey's Honestly Significant Difference (HSD), Bonferroni correction, Scheffe's test, and Dunnett's test. Tukey's HSD is often used when the number of groups is equal and the sample sizes are equal, and it controls the family-wise error rate. Bonferroni correction is often used when there are multiple pairwise comparisons, and it controls the overall type I error rate. Scheffe's test is used when the number of groups and sample sizes are unequal, and it controls the family-wise error rate. Dunnett's test is used when one group is compared to all other groups, and it controls the overall type I error rate.\n",
    "\n",
    "A post-hoc test might be necessary when the ANOVA reveals a significant difference between at least two groups. For example, if an ANOVA comparing the mean weight loss of three diets reveals a significant difference, a post-hoc test could be used to determine which specific pairs of diets differ significantly.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9:-\n",
    "\n",
    " Here is an example of conducting a one-way ANOVA in Python to compare the mean weight loss of three diets:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# create data\n",
    "data = pd.DataFrame({'diet': ['A', 'B', 'C', 'A', 'B', 'C', ...],\n",
    "                     'weight_loss': [3.5, 4.2, 5.1, 2.9, 3.8, 4.9, ...]})\n",
    "\n",
    "# conduct ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(data[data['diet'] == 'A']['weight_loss'],\n",
    "                                      data[data['diet'] == 'B']['weight_loss'],\n",
    "                                      data[data['diet'] == 'C']['weight_loss'])\n",
    "\n",
    "# print results\n",
    "print('F-statistic:', f_statistic)\n",
    "print('p-value:', p_value)\n",
    "```\n",
    "\n",
    "Assuming the data is normally distributed and the assumptions of ANOVA are met, the output of the above code will provide the F-statistic and p-value. The F-statistic measures the ratio of the variance between the groups to the variance within the groups, and the p-value indicates the probability of observing an F-statistic as extreme or more extreme than the one calculated, assuming there is no difference between the groups. \n",
    "\n",
    "If the p-value is less than the chosen alpha level (e.g., 0.05), we can reject the null hypothesis and conclude that there is a significant difference between the mean weight loss of at least two diets. The interpretation of the results should be done in conjunction with the effect size measures and post-hoc tests to determine which specific pairs of diets differ significantly from each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10:-\n",
    "\n",
    "\n",
    "\n",
    "To conduct a two-way ANOVA in Python, we will first need to import the necessary libraries:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "```\n",
    "\n",
    "Assuming we have the data in a CSV file named \"data.csv,\" we can read it into a Pandas DataFrame:\n",
    "\n",
    "```python\n",
    "data = pd.read_csv('data.csv')\n",
    "```\n",
    "\n",
    "The data should have four columns: \"program\" (A, B, or C), \"experience\" (novice or experienced), \"time\" (the time it took to complete the task), and \"employee_id\" (a unique identifier for each employee).\n",
    "\n",
    "Next, we will fit a two-way ANOVA model using the following formula:\n",
    "\n",
    "```python\n",
    "model = ols('time ~ C(program) + C(experience) + C(program):C(experience)', data=data).fit()\n",
    "```\n",
    "\n",
    "This formula includes main effects for both \"program\" and \"experience,\" as well as an interaction effect between the two. The \"C\" function is used to indicate that these variables should be treated as categorical variables.\n",
    "\n",
    "We can then print a summary of the model to see the F-statistics and p-values:\n",
    "\n",
    "```python\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "This will output a table that includes the F-statistics and p-values for each of the main effects and the interaction effect.\n",
    "\n",
    "To interpret the results, we will focus on the p-values. A p-value less than 0.05 indicates that the effect is statistically significant. If we see a significant main effect for \"program,\" this means that there is a significant difference in the average time it takes to complete the task using the different software programs, regardless of experience level. Similarly, if we see a significant main effect for \"experience,\" this means that there is a significant difference in the average time it takes to complete the task between novice and experienced employees, regardless of software program. \n",
    "\n",
    "If we see a significant interaction effect between \"program\" and \"experience,\" this means that the effect of the software program on task completion time differs depending on whether the employee is a novice or experienced user. In other words, the difference in completion time between the programs may be greater or smaller for experienced employees compared to novice employees.\n",
    "\n",
    "Overall, a two-way ANOVA can provide valuable insights into the factors that influence task completion time, and can help organizations make data-driven decisions about software program selection and employee training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11:-\n",
    "\n",
    "\n",
    "\n",
    "To conduct a two-sample t-test in Python, we can use the `scipy.stats` module. Here's how we can perform the analysis:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Generate some example data\n",
    "control_scores = np.random.normal(70, 10, size=100)\n",
    "experimental_scores = np.random.normal(75, 10, size=100)\n",
    "\n",
    "# Perform the two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "# Print the results\n",
    "print(\"t-statistic: \", t_statistic)\n",
    "print(\"p-value: \", p_value)\n",
    "```\n",
    "\n",
    "In this example, we've generated some example data to simulate the test scores for the control and experimental groups. We've then used the `ttest_ind` function from the `scipy.stats` module to perform the two-sample t-test. The `ttest_ind` function takes two arrays of data as input and returns the t-statistic and p-value.\n",
    "\n",
    "The output will give us the t-statistic and the p-value for the two-sample t-test. If the p-value is less than our chosen level of significance (e.g. 0.05), we can conclude that there is a significant difference in test scores between the two groups.\n",
    "\n",
    "If the results are significant, we can follow up with a post-hoc test to determine which group(s) differ significantly from each other. There are various post-hoc tests available, but one common method is the Tukey's Honestly Significant Difference (HSD) test, which can be performed using the `statsmodels` module. Here's an example:\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Combine the data from the two groups\n",
    "all_scores = np.concatenate([control_scores, experimental_scores])\n",
    "\n",
    "# Create a list of group labels\n",
    "group_labels = [\"Control\"] * len(control_scores) + [\"Experimental\"] * len(experimental_scores)\n",
    "\n",
    "# Perform the Tukey HSD test\n",
    "tukey_results = pairwise_tukeyhsd(all_scores, group_labels)\n",
    "\n",
    "# Print the results\n",
    "print(tukey_results)\n",
    "```\n",
    "\n",
    "In this example, we've combined the data from both groups and created a list of group labels. We've then used the `pairwise_tukeyhsd` function from the `statsmodels` module to perform the Tukey HSD test. The `pairwise_tukeyhsd` function takes the combined data and group labels as input and returns a table of results showing which group(s) differ significantly from each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
