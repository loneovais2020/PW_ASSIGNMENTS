{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.**\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique that transforms the values of each feature in a dataset to a range of [0, 1]. This is done by subtracting the minimum value of each feature from all of its values, and then dividing the result by the difference between the maximum and minimum values of the feature.\n",
    "\n",
    "Min-Max scaling is often used to normalize features before training a machine learning model. This is because different features can have different scales, and this can make it difficult for a model to learn effectively. By normalizing the features, the model can learn more easily from the data.\n",
    "\n",
    "For example, let's say we have a dataset with two features: price and rating. The price feature ranges from \\$10 to \\$100, while the rating feature ranges from 1 to 5. If we don't scale the features, the model will have to learn two different scales, which can make it more difficult to learn.\n",
    "\n",
    "However, if we scale the features using Min-Max scaling, the price feature will range from 0 to 1, and the rating feature will range from 0 to 1. This will make it easier for the model to learn from the data, and it will improve the model's performance.\n",
    "\n",
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n",
    "\n",
    "The Unit Vector technique is a data preprocessing technique that transforms the values of each feature in a dataset to a unit vector. This is done by dividing each feature by its magnitude.\n",
    "\n",
    "The Unit Vector technique is similar to Min-Max scaling in that it transforms the values of each feature to a common scale. However, the Unit Vector technique does not have the same range as Min-Max scaling. The Unit Vector technique will always scale the values of each feature to a magnitude of 1, while Min-Max scaling can scale the values of each feature to any range.\n",
    "\n",
    "For example, let's say we have a dataset with two features: price and rating. The price feature ranges from \\$10 to \\$100, while the rating feature ranges from 1 to 5. If we scale the features using the Unit Vector technique, the price feature will have a magnitude of 10, and the rating feature will have a magnitude of 1.\n",
    "\n",
    "This means that the price feature will be 10 times more important than the rating feature when training a machine learning model. This is because the price feature has a much larger magnitude than the rating feature.\n",
    "\n",
    "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**\n",
    "\n",
    "PCA (Principle Component Analysis) is a dimensionality reduction technique that transforms a dataset of features into a new dataset with fewer features. This is done by finding the principal components of the dataset, which are the directions in which the data varies the most.\n",
    "\n",
    "PCA is often used to reduce the dimensionality of datasets before training a machine learning model. This is because high-dimensional datasets can be difficult for models to learn from. By reducing the dimensionality of the dataset, the model can learn more easily from the data.\n",
    "\n",
    "For example, let's say we have a dataset with 100 features. This dataset is too high-dimensional for most models to learn from effectively. However, if we use PCA to reduce the dimensionality of the dataset to 10 features, the model will be able to learn from the data more effectively.\n",
    "\n",
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.**\n",
    "\n",
    "PCA can be used for feature extraction by identifying the principal components that explain the most variance in the data. These principal components can then be used as features for a machine learning model.\n",
    "\n",
    "For example, let's say we have a dataset with two features: price and rating. The price feature explains 80% of the variance in the data, while the rating feature explains 20% of the variance in the data.\n",
    "\n",
    "If we use PCA to reduce the dimensionality of the dataset to one feature, the principal component that will be chosen is the price feature. This is because the price feature explains the most variance in the data.\n",
    "\n",
    "\n",
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.**\n",
    "\n",
    "To build a recommendation system for a food delivery service, I would use Min-Max scaling to preprocess the data. Min-Max scaling is a technique that normalizes the values of each feature in a dataset to a range of [0, 1]. This is done by subtracting the minimum value of each feature from all of its values, and then dividing the result by the difference between the maximum and minimum values of the feature.\n",
    "\n",
    "In this case, I would use Min-Max scaling to normalize the values of the price, rating, and delivery time features. This would make it easier for the recommendation system to learn from the data, and it would improve the system's performance.\n",
    "\n",
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.**\n",
    "\n",
    "To build a model to predict stock prices, I would use PCA to reduce the dimensionality of the dataset. PCA is a technique that transforms a dataset of features into a new dataset with fewer features. This is done by finding the principal components of the dataset, which are the directions in which the data varies the most.\n",
    "\n",
    "In this case, I would use PCA to reduce the dimensionality of the dataset to a few key features. This would make it easier for the model to learn from the data, and it would improve the model's performance.\n",
    "\n",
    "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.**\n",
    "\n",
    "To perform Min-Max scaling on the dataset containing the values [1, 5, 10, 15, 20], I would follow these steps:\n",
    "\n",
    "1. Find the minimum and maximum values in the dataset.\n",
    "2. Subtract the minimum value from each value in the dataset.\n",
    "3. Divide each value in the dataset by the difference between the maximum and minimum values.\n",
    "\n",
    "The resulting values will be in the range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9999999999999999, -0.5789473684210525, -0.05263157894736836, 0.47368421052631593, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape the data to a 2D array as MinMaxScaler expects it\n",
    "data = [[value] for value in data]\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Reshape the scaled data back to a 1D array\n",
    "scaled_data = [value[0] for value in scaled_data]\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?**\n",
    "\n",
    "To perform Feature Extraction using PCA on the dataset containing the features [height, weight, age, gender, blood pressure], I would follow these steps:\n",
    "\n",
    "1. Calculate the covariance matrix of the dataset.\n",
    "2. Find the eigenvalues and eigenvectors of the covariance matrix.\n",
    "3. Sort the eigenvalues in descending order.\n",
    "4. Choose the top k eigenvectors, where k is the number of principal components that you want to retain.\n",
    "\n",
    "The resulting principal components will be a new dataset with fewer features. The number of principal components that you choose to retain will depend on the specific application. In general, you want to choose as many principal components as necessary to capture the majority of the variance in the data.\n",
    "\n",
    "In this case, I would choose to retain the top 2 principal components. This is because the first 2 principal components account for 95% of the variance in the data. This means that the new dataset with 2 principal components will still contain most of the information in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [9.92060436e-01 5.90120461e-03 1.90571111e-03 1.32648574e-04]\n",
      "Cumulative explained variance ratio: [0.99206044 0.99796164 0.99986735 1.        ]\n",
      "Number of principal components to retain: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "dataset = np.array([[170, 65, 30, 1, 120],\n",
    "                    [160, 55, 25, 0, 118],\n",
    "                    [175, 70, 35, 1, 122],\n",
    "                    [180, 80, 40, 1, 130],\n",
    "                    [165, 60, 28, 0, 115]])\n",
    "\n",
    "# Separate the features (X) from the target variable (y)\n",
    "X = dataset[:, :-1]\n",
    "\n",
    "# Create an instance of PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the PCA model on the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Calculate the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "num_components = np.argmax(cumulative_explained_variance_ratio >= 0.95) + 1\n",
    "\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative explained variance ratio:\", cumulative_explained_variance_ratio)\n",
    "print(\"Number of principal components to retain:\", num_components)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
